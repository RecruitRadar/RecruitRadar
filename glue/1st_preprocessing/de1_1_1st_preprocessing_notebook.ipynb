{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 3.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"\n",
				"import json\n",
				"import boto3\n",
				"import requests\n",
				"from typing import List\n",
				"from datetime import date\n",
				"from pyspark.sql import SparkSession, DataFrame\n",
				"from pyspark.sql.types import StringType, StructField, StructType, ArrayType, FloatType\n",
				"from pyspark.sql.functions import udf, regexp_replace, explode, col, to_date, concat, row_number, lit\n",
				"from pyspark.sql.window import Window\n",
				"\n",
				"from botocore.exceptions import NoCredentialsError\n",
				"from botocore.exceptions import ClientError\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"class S3Uploader:\n",
				"    def __init__(self, bucket_name, access_key, secret_key, region_name):\n",
				"        self.bucket_name = bucket_name\n",
				"        self.access_key = access_key\n",
				"        self.secret_key = secret_key\n",
				"        self.s3 = boto3.client(\n",
				"            's3',\n",
				"            aws_access_key_id=access_key,\n",
				"            aws_secret_access_key=secret_key,\n",
				"            region_name=region_name\n",
				"        )\n",
				"\n",
				"    def get_upload_file_path(self):\n",
				"        \"\"\"\n",
				"        1차 전처리된 최종 parquet 파일 및 parquet crc 파일을 s3에 업로드할 경로를 리턴합니다.\n",
				"        \"\"\"\n",
				"        today = date.today()\n",
				"        year = str(today.year)\n",
				"        month = str(today.month).zfill(2)\n",
				"        day = str(today.day).zfill(2)\n",
				"\n",
				"        return f's3://{self.bucket_name}/1st_cleaned_data/year={year}/month={month}/day={day}'\n",
				"    \n",
				"    def delete_crc_files(self):\n",
				"        \"\"\"\n",
				"        s3에 올라간 parquet crc 파일을 제거합니다.\n",
				"        \"\"\"\n",
				"        today = date.today()\n",
				"        year = str(today.year)\n",
				"        month = str(today.month).zfill(2)\n",
				"        day = str(today.day).zfill(2)\n",
				"\n",
				"        remote_path = f'1st_cleaned_data/year={year}/month={month}/day={day}'\n",
				"\n",
				"        response = self.s3.list_objects(Bucket=self.bucket_name, Prefix=remote_path)\n",
				"\n",
				"        for obj in response.get('Contents', []):\n",
				"            file_key = obj['Key']\n",
				"            if file_key.endswith('.crc'):\n",
				"                try:\n",
				"                    self.s3.delete_object(Bucket=self.bucket_name, Key=file_key)\n",
				"                except NoCredentialsError:\n",
				"                    raise Exception(\"AWS credentials not available\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Function to create DynamicFrame from the catalog\n",
				"def create_dyf_from_catalog(database_name, table_name):\n",
				"    today = date.today()\n",
				"    year = str(today.year)\n",
				"    month = str(today.month).zfill(2)\n",
				"    day = str(today.day).zfill(2)\n",
				"    return glueContext.create_dynamic_frame.from_catalog(\n",
				"        database=database_name,\n",
				"        table_name=table_name,\n",
				"        push_down_predicate=f'(year==\"{year}\" and month==\"{month}\" and day==\"{day}\")'\n",
				"    )\n",
				"\n",
				"def unnest_and_rename(df):\n",
				"    # Unnest the 'results' column\n",
				"    df_unnested = df.select(\"*\", explode(df.results).alias(\"unnested_results\")).drop(\"results\")\n",
				"    \n",
				"    # Selecting and naming nested fields\n",
				"    df_final = df_unnested.select(\n",
				"        \"unnested_results.job_id\",\n",
				"        \"unnested_results.platform\",\n",
				"        \"unnested_results.category\",\n",
				"        \"unnested_results.url\",\n",
				"        \"unnested_results.company\",\n",
				"        \"unnested_results.title\",\n",
				"        \"unnested_results.primary_responsibility\",\n",
				"        \"unnested_results.required\",\n",
				"        \"unnested_results.preferred\",\n",
				"        \"unnested_results.end_at\",\n",
				"        \"unnested_results.skills\",\n",
				"        \"unnested_results.location\",\n",
				"        \"unnested_results.welfare\",\n",
				"        \"unnested_results.body\",\n",
				"        \"unnested_results.company_description\",\n",
				"        \"unnested_results.coordinate\",\n",
				"        \"year\",\n",
				"        \"month\",\n",
				"        \"day\"\n",
				"    )\n",
				"    \n",
				"    return df_final\n",
				"\n",
				"def rearrange_dataframe_columns(df: DataFrame, columns: List[str]) -> DataFrame:\n",
				"        return df.select(*columns)\n",
				"    \n",
				"def process_text_columns(df: DataFrame, columns: List[str]) -> DataFrame:\n",
				"    for col in columns:\n",
				"        df = df.withColumn(col, regexp_replace(df[col], \"\\n\", \" \"))\n",
				"    return df\n",
				"\n",
				"\n",
				"def get_secret():\n",
				"    \"\"\"\n",
				"    AWS Secrets Manager를 이용해 환경변수를 불러옵니다.\n",
				"    \"\"\"\n",
				"    secret_name = \"prod/de-1-1/back-end\"\n",
				"    REGION_NAME = \"ap-northeast-2\"\n",
				"\n",
				"    session = boto3.session.Session()\n",
				"    client = session.client(\n",
				"        service_name='secretsmanager',\n",
				"        region_name=REGION_NAME\n",
				"    )\n",
				"\n",
				"    try:\n",
				"        get_secret_value_response = client.get_secret_value(\n",
				"            SecretId=secret_name\n",
				"        )\n",
				"    except ClientError as e:\n",
				"        raise e\n",
				"\n",
				"    secret = get_secret_value_response['SecretString']\n",
				"    secret_dict = json.loads(secret)\n",
				"\n",
				"    BUCKET_NAME = secret_dict['BUCKET_NAME']\n",
				"    ACCESS_KEY = secret_dict['AWS_ACCESS_KEY_ID']\n",
				"    SECRET_KEY = secret_dict['AWS_SECRET_ACCESS_KEY']\n",
				"    KAKAO_API_TOKEN = secret_dict['KAKAO_API_TOKEN']\n",
				"\n",
				"    return BUCKET_NAME, ACCESS_KEY, SECRET_KEY, REGION_NAME, KAKAO_API_TOKEN\n",
				"\n",
				"BUCKET_NAME, ACCESS_KEY, SECRET_KEY, REGION_NAME, KAKAO_API_TOKEN = get_secret()\n",
				"\n",
				"@udf(returnType=ArrayType(FloatType()))\n",
				"def get_coordinate_from_location(location, KAKAO_API_TOKEN=KAKAO_API_TOKEN):\n",
				"    headers = {'Authorization': 'KakaoAK ' + KAKAO_API_TOKEN}\n",
				"    \n",
				"    \"\"\"\n",
				"    카카오 API를 통해 location으로부터 coordinate(lat, lon) 리스트를 반환합니다.\n",
				"    \"\"\"\n",
				"    if location is None:\n",
				"        return None\n",
				"\n",
				"    url = f'https://dapi.kakao.com/v2/local/search/address.json?query={location}'\n",
				"\n",
				"    try:\n",
				"        response = requests.get(url, headers=headers, timeout=5)\n",
				"        result = json.loads(response.text)\n",
				"        match_first = result['documents'][0]['address']\n",
				"        return [float(match_first['y']), float(match_first['x'])]\n",
				"\n",
				"    except (requests.exceptions.RequestException, TypeError, ValueError, KeyError, IndexError) as e:\n",
				"        print(f'Error occurred: {e} while fetching address: {location}')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Initialize the Spark and Glue contexts\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = (glueContext.spark_session.builder\n",
				"         .appName(\"Python Spark preprocessing #1\")\n",
				"         .master(\"local\")\n",
				"         .getOrCreate())\n",
				"spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") #success 메타 파일\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"database_name = 'de1_1_database'\n",
				"\n",
				"# Create DynamicFrames using the function\n",
				"jobplanet_dyf = create_dyf_from_catalog(database_name, \"jobplanet\")\n",
				"rallit_dyf = create_dyf_from_catalog(database_name, \"rallit\")\n",
				"jumpit_dyf = create_dyf_from_catalog(database_name, \"jumpit\")\n",
				"wanted_dyf = create_dyf_from_catalog(database_name, \"wanted\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Convert DynamicFrames to Spark DataFrames\n",
				"jobplanet_df = jobplanet_dyf.toDF()\n",
				"rallit_df = rallit_dyf.toDF()\n",
				"jumpit_df = jumpit_dyf.toDF()\n",
				"wanted_df = wanted_dyf.toDF()\n",
				"\n",
				"# Apply the refactored function\n",
				"#jobplanet_df = unnest_and_rename(jobplanet_df)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# 스키마를 rallit_df 스키마와 동일하게 조정\n",
				"wanted_df = wanted_df.select(rallit_df.columns)\n",
				"jumpit_df = jumpit_df.select(rallit_df.columns)\n",
				"jobplanet_df = jobplanet_df.select(rallit_df.columns)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Union the DataFrames\n",
				"df_final = rallit_df.union(wanted_df).union(jobplanet_df).union(jumpit_df)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# 대분류, 중분류, 카테고리를 나타내는 데이터\n",
				"categories = [\n",
				"    (\"WEB\", \"서버/백엔드 개발자\", [\"서버 개발자\", \"자바 개발자\", \"Node.js 개발자\", \"PHP 개발자\", \"웹 개발자\", \"루비온레일즈 개발자\", \".NET 개발자\", \"백엔드 개발\", \"웹개발\", \"BACKEND_DEVELOPER\", \"서버/백엔드 개발자\", \"웹 풀스택 개발자\"]),\n",
				"    (\"WEB\", \"프론트엔드 개발자\", [\"프론트엔드 개발자\",\"프론트엔드 개발\",\"FRONTEND_DEVELOPER\"]),\n",
				"    (\"WEB\", \"웹 퍼블리셔\", [\"웹 퍼블리셔\",\"웹퍼블리셔\"]),\n",
				"    (\"GAME\", \"게임 개발자\", [\"게임개발\", \"게임 클라이언트 개발자\", \"게임 서버 개발자\"]),\n",
				"    (\"GAME\", \"VR/AR/3D\", [\"VR 엔지니어\", \"그래픽스 엔지니어\", \"VR/AR/3D,게임 클라이언트 개발자\"]),\n",
				"    (\"DATA\", \"데이터 사이언티스트\", [\"데이터 사이언티스트\", \"DATA_SCIENTIST\"]),\n",
				"    (\"DATA\", \"데이터 엔지니어\", [\"데이터 엔지니어\", \"빅데이터 엔지니어\", \"DATA_ENGINEER\"]),\n",
				"    (\"DATA\", \"데이터 분석가\", [\"BI 엔지니어\", \"데이터 분석가\", \"DATA_ANALYST\"]),\n",
				"    (\"DATA\", \"AI 엔지니어\", [\"머신러닝 엔지니어\", \"영상,음성 엔지니어\", \"MACHINE_LEARNING\", \"인공지능/머신러닝\"]),\n",
				"    (\"DATA\", \"DBA\", [\"DBA\", \"빅데이터 엔지니어,DBA\"]),\n",
				"    (\"MOBILE\", \"안드로이드 개발자\", [\"안드로이드 개발자\", \"안드로이드 개발\", \"ANDROID_DEVELOPER\"]),\n",
				"    (\"MOBILE\", \"iOS 개발자\", [\"iOS 개발자\", \"iOS\", \"IOS_DEVELOPER\", \"IOS 개발자\"]),\n",
				"    (\"MOBILE\", \"크로스 플랫폼 모바일 개발자\", [\"크로스플랫폼 앱 개발자\", \"크로스플랫폼 앱개발자\", \"CROSS_PLATFORM_DEVELOPER\"]),\n",
				"    (\"SUPPORT\", \"PM\", [\"개발 매니저\", \"프로덕트 매니저\", \"AGILE_SCRUM_MASTER\", \"인공지능/머신러닝,개발 PM\"]),\n",
				"    (\"SUPPORT\", \"QA 엔지니어\", [\"QA,테스트 엔지니어\", \"QA\", \"QA_ENGINEER\", \"QA 엔지니어\"]),\n",
				"    (\"SUPPORT\", \"기술지원\", [\"기술지원\", \"SUPPORT_ENGINEER\"]),\n",
				"    (\"DEVSECOPS\", \"데브옵스/인프라 엔지니어\", [\"DevOps / 시스템 관리자\", \"시스템,네트워크 관리자\", \"네트워크/보안/운영\", \"클라우드 개발\", \"DEV_OPS\", \"INFRA_ENGINEER\", \"devops/시스템 엔지니어\"]),\n",
				"    (\"DEVSECOPS\", \"정보보안 담당자\", [\"보안 엔지니어\", \"CIO,Chief Information Officer\", \"SECURITY_ENGINEER\", \"정보보안 담당자\"]),\n",
				"    (\"SW/HW/IOT\", \"HW/임베디드 개발자\", [\"임베디드 개발자\", \"하드웨어 엔지니어\", \"하드웨어 개발\", \"HARDWARE_EMBEDDED_ENGINEER\", \"HW/임베디드\"]),\n",
				"    (\"SW/HW/IOT\", \"소프트웨어 개발자\", [\"소프트웨어 엔지니어\", \"파이썬 개발자\", \"C,C++ 개발자\", \"소프트웨어 개발\", \"소프트웨어아키텍트\", \"SOFTWARE_ENGINEER\", \"SW/솔루션\"]),\n",
				"    (\"ETC\", \"블록체인 엔지니어\", [\"블록체인 플랫폼 엔지니어\", \"BLOCKCHAIN_ENGINEER\", \"프론트엔드 개발자,블록체인\"]),\n",
				"    (\"ETC\", \"기타\", [\"ERP전문가\", \"CTO,Chief Technology Officer\", \"CTO\", \"ERP\", \"etc\"])\n",
				"]\n",
				"\n",
				"data_list = []\n",
				"for major_category, middle_category, job_list in categories:\n",
				"    for sub_category in job_list:\n",
				"        data_list.append((major_category, middle_category, sub_category))\n",
				"\n",
				"schema = StructType([\n",
				"    StructField(\"major_category\", StringType(), True),\n",
				"    StructField(\"middle_category\", StringType(), True),\n",
				"    StructField(\"sub_category\", StringType(), True)\n",
				"])\n",
				"mapping_df = spark.createDataFrame(data_list, schema=schema)\n",
				"df_with_mapped_categories = df_final.join(mapping_df, df_final.category == mapping_df.sub_category, \"left\")\n",
				"\n",
				"new_columns = [\n",
				"        'job_id','platform', 'category', 'major_category', 'middle_category', \\\n",
				"        'sub_category', 'company', 'title', 'preferred', 'required', 'primary_responsibility', \\\n",
				"        'url', 'end_at', 'skills', 'location', 'welfare', 'body', 'company_description', 'coordinate'\n",
				"    ]\n",
				"\n",
				"df_with_mapped_categories = rearrange_dataframe_columns(df_with_mapped_categories, new_columns)\n",
				"df_with_mapped_categories = rearrange_dataframe_columns(df_with_mapped_categories, new_columns)\n",
				"\n",
				"text_columns = ['preferred', 'required', 'primary_responsibility', 'welfare', 'company_description']\n",
				"\n",
				"df_final = process_text_columns(df_with_mapped_categories, text_columns)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"df_filter_for_wanted_rallit = df_final.filter((df_final['platform'] == 'wanted') | (df_final['platform'] == 'rallit'))\n",
				"df_filter_for_jobplanet_jumpit = df_final.filter((df_final['platform'] == 'jobplanet') | (df_final['platform'] == 'jumpit'))\n",
				"\n",
				"df_with_coordinate = df_filter_for_jobplanet_jumpit.withColumn('coordinate', get_coordinate_from_location('location'))\n",
				"\n",
				"result_df = df_filter_for_wanted_rallit.union(df_with_coordinate)\n",
				"\n",
				"result_repartitioned_df = result_df.repartition(1)\n",
				"\n",
				"uploader = S3Uploader(BUCKET_NAME, ACCESS_KEY, SECRET_KEY, REGION_NAME)\n",
				"upload_file_path = uploader.get_upload_file_path()\n",
				"result_repartitioned_df.write.parquet(upload_file_path, mode=\"overwrite\")\n",
				"uploader.delete_crc_files()\n",
				"\n",
				"# glueContext.stop() -> Glue는 job.commit으로 stop 역할을 합니다.\n",
				"job.commit()\n",
				"sc.stop()"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
